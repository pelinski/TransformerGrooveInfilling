{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "\n",
    "from onsets import input_features_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 16])\n"
     ]
    }
   ],
   "source": [
    "filters = {\n",
    "    \"drummer\" : None,\n",
    "    \"session\" : None,\n",
    "    \"loop_id\" : None, \n",
    "    \"master_id\" : None,\n",
    "    \"style_primary\" : None,\n",
    "    \"bpm\"  : [120],\n",
    "    \"beat_type\" :[\"beat\"],\n",
    "    \"time_signature\" : [\"4-4\"],\n",
    "    \"full_midi_filename\"  : None,\n",
    "    \"full_audio_filename\": None\n",
    "}\n",
    "\n",
    "sr = 44100\n",
    "FRAME_INTERVAL = 0.01\n",
    "hop_length =  int(round(FRAME_INTERVAL * sr))\n",
    "\n",
    "input_features_parameters = {\n",
    "    \"sr\" : sr,\n",
    "    \"n_fft\" : 1024,\n",
    "    \"win_length\" : 1024,\n",
    "    \"hop_length\" : hop_length,\n",
    "    \"n_bins_per_octave\" : 16,\n",
    "    \"n_octaves\" : 9,\n",
    "    \"f_min\" : 40,\n",
    "    \"mean_filter_size\" : 22,\n",
    "    \"n_bars\" : 2, \n",
    "    \"time_signature_numerator\" : 4, \n",
    "    \"time_signature_denominator\" : 4, \n",
    "    \"beat_division_factors\" : [4],\n",
    "    \"qpm\" : 120\n",
    "}\n",
    "\n",
    "a = get_input_features(\"./misc/temp.wav\", **input_features_parameters)\n",
    "print(a.shape) #34? should be 32?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_passes_filters(obj,filters):\n",
    "    for key in filters:\n",
    "        if filters[key] is not None and obj.to_dict()[key] not in filters[key]:\n",
    "            return False        \n",
    "    return True\n",
    "\n",
    "class GrooveMidiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                source_path = '../processed_dataset/hvo_0.2.0/Processed_On_09_05_2021_at_23_06_hrs',\n",
    "                subset = 'GrooveMIDI_processed_test',\n",
    "                metadata_csv_filename='metadata.csv',\n",
    "                hvo_pickle_filename='hvo_sequence_data.obj',\n",
    "                filters=filters,\n",
    "                input_features_parameters=input_features_parameters,\n",
    "                synthesized_audio_path = '../synthesized_audio/hvo_0.2.0/Processed_On_09_05_2021_at_23_06_hrs',\n",
    "                sf_path = \"../hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\",\n",
    "                max_len= 32):\n",
    "        train_file = open(os.path.join(source_path, subset, hvo_pickle_filename),'rb')\n",
    "        train_set = pickle.load(train_file)\n",
    "        metadata = pd.read_csv(os.path.join(source_path, subset, metadata_csv_filename))\n",
    "        \n",
    "        self.hvo_sequences=[]\n",
    "        self.processed_inputs=[]\n",
    "        self.processed_outputs=[]\n",
    "        \n",
    "        for ix, hvo_seq in enumerate(train_set):\n",
    "            if len(hvo_seq.time_signatures) == 1: # ignore if time_signature change happens\n",
    "                all_zeros = not np.any(hvo_seq.hvo.flatten())\n",
    "                if not all_zeros: # ignore silent patterns\n",
    "                    if check_if_passes_filters(metadata.loc[ix], filters):\n",
    "                                                \n",
    "                        #add metadata to hvo_seq scores\n",
    "                        hvo_seq.drummer = metadata.loc[ix].at[\"drummer\"]\n",
    "                        hvo_seq.session = metadata.loc[ix].at[\"session\"]\n",
    "                        hvo_seq.master_id = metadata.loc[ix].at[\"master_id\"]\n",
    "                        hvo_seq.style_primary = metadata.loc[ix].at[\"style_primary\"]\n",
    "                        hvo_seq.style_secondary = metadata.loc[ix].at[\"style_secondary\"]\n",
    "                        hvo_seq.beat_type = metadata.loc[ix].at[\"beat_type\"]\n",
    "                        hvo_seq.loop_id = metadata.loc[ix].at[\"loop_id\"]\n",
    "                        \n",
    "                        # pad with zeros to match max_len\n",
    "                        pad_count = max(max_len - hvo_seq.hvo.shape[0],0)\n",
    "                        hvo_seq.hvo = np.pad(hvo_seq.hvo, ((0,pad_count), (0,0)), 'constant')\n",
    "                        hvo_seq.hvo = hvo_seq.hvo[:max_len, :] # in case seq exceeds max len\n",
    "                        self.hvo_sequences.append(hvo_seq)\n",
    "                        \n",
    "                        # get processed inputs\n",
    "                        audio_file_path = os.path.join(synthesized_audio_path, subset, hvo_seq.master_id)\n",
    "                        if not os.path.exists(audio_file_path): os.makedirs(audio_file_path)\n",
    "                        filename = os.path.join(audio_file_path, hvo_seq.loop_id.split('/')[-1].replace(':','_')+'.wav')\n",
    "                        hvo_seq.save_audio(filename,sr,sf_path)\n",
    "                        \n",
    "                        input_features = input_features_extractor(filename, **input_features_parameters)\n",
    "                        self.processed_inputs.append(input_features)\n",
    "                        \n",
    "                        #get processed_outputs\n",
    "                        # iteration 1\n",
    "                        \n",
    "                        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hvo_sequences)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.hvo_sequences[idx].hvo, idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmd = GrooveMidiDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmd.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_thesis",
   "language": "python",
   "name": "torch_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../model')\n",
    "sys.path.append('../../preprocessed_dataset/')\n",
    "import math\n",
    "\n",
    "from dataset_loader import GrooveMidiDataset\n",
    "from Subset_Creators.subsetters import GrooveMidiSubsetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model) # shape (max_len=5000, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float) # Shape (max_len=5000)\n",
    "        position = position.unsqueeze(1) # Shape (5000, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Shape (d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        if d_model % 2 is not 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]\n",
    "        else: \n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Insert a new dimension for batch size\n",
    "        pe = pe.unsqueeze(0) # Shape (1, 5000, d_model)\n",
    "        pe = pe.transpose(0, 1) # Shape (5000, 1, d_model) \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer (torch.nn.Module):\n",
    "    def __init__(self, d_model_enc, d_model_dec, nhead_enc,nhead_dec, dim_feedforward, dropout, num_encoder_layers, num_decoder_layers,max_len):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.PositionalEncoder = PositionalEncoding(d_model_enc, dropout, max_len)\n",
    "        \n",
    "        norm_encoder = torch.nn.LayerNorm(d_model_enc)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model_enc, nhead_enc, dim_feedforward,dropout)\n",
    "        self.Encoder = torch.nn.TransformerEncoder(encoder_layer, num_encoder_layers, norm_encoder)\n",
    "        \n",
    "        self.MemoryMap = torch.nn.Linear(d_model_enc, d_model_dec, bias=False)\n",
    "        \n",
    "        norm_decoder = torch.nn.LayerNorm(d_model_dec)\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model_dec, nhead_dec, dim_feedforward, dropout)\n",
    "        self.Decoder = torch.nn.TransformerDecoder(decoder_layer, num_decoder_layers, norm_decoder)\n",
    "        \n",
    "    def forward(self, src=None, tgt=None, _mem=None, only_encoder=False, only_decoder=False):\n",
    "        \n",
    "        x = self.PositionalEncoder(src)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if only_encoder:\n",
    "            memory = self.Encoder(x)\n",
    "            return memory\n",
    "        \n",
    "        if only_decoder: \n",
    "            mask = (torch.triu(torch.ones(tgt.shape[0], tgt.shape[0])) == 1)             # future mask\n",
    "            out = self.Decoder(tgt,_mem, tgt_mask=mask)\n",
    "            out = torch.reshape(out, (tgt_len, N, 3, d_model // 3))\n",
    "            return out\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"x\", x.shape)\n",
    "        memory = self.Encoder(x)\n",
    "        print(\"memory\", memory.shape)\n",
    "        memory_map = self.MemoryMap(memory)\n",
    "        print(\"memory_map\", memory_map.shape)\n",
    "\n",
    "        mask = (torch.triu(torch.ones(tgt.shape[0], tgt.shape[0])) == 1)             # future mask\n",
    "        print(\"mask\", mask.shape)\n",
    "        out = self.Decoder(tgt, memory_map, tgt_mask=mask)\n",
    "        print(\"out\", out.shape)\n",
    "        out = torch.reshape(out, (tgt.shape[0], tgt.shape[1], 3, d_model_dec // 3))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([12, 24, 27])\n",
      "memory torch.Size([12, 24, 27])\n",
      "memory_map torch.Size([12, 24, 27])\n",
      "mask torch.Size([12, 12])\n",
      "out torch.Size([12, 24, 27])\n",
      "torch.Size([12, 24, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "d_model_enc = 27\n",
    "d_model_dec = 27\n",
    "nhead_enc = 3\n",
    "nhead_dec = 3\n",
    "dim_feedforward = d_model*10\n",
    "dropout = 0.1\n",
    "num_encoder_layers = 5\n",
    "num_decoder_layers = 6\n",
    "max_len=32\n",
    "\n",
    "\n",
    "\n",
    "TM = Transformer(d_model_enc,d_model_dec, nhead_enc, nhead_dec, dim_feedforward, dropout, num_encoder_layers, num_decoder_layers,max_len)\n",
    "\n",
    "N = 24\n",
    "src_len = 12\n",
    "tgt_len = 12\n",
    "\n",
    "src = torch.rand(src_len, N, d_model_enc)\n",
    "tgt = torch.rand(tgt_len, N, d_model_dec)\n",
    "\n",
    "print(TM.forward(src,tgt).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test gmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"beat_type\": [\"beat\"],\n",
    "           \"time_signature\" : [\"4-4\"],\n",
    "           \"master_id\":[\"drummer9/session1/9\"]}\n",
    "\n",
    "mso_parameters = {\"sr\": 44100,\n",
    "                  \"n_fft\": 1024,\n",
    "                  \"win_length\": 1024,\n",
    "                  \"hop_length\": 441,\n",
    "                  \"n_bins_per_octave\": 16,\n",
    "                  \"n_octaves\": 9,\n",
    "                  \"f_min\": 40,\n",
    "                  \"mean_filter_size\": 22\n",
    "                 }\n",
    "\n",
    "voices_parameters = {\"voice_idx\": [2], # closed hihat\n",
    "                     \"min_n_voices_to_remove\": 1,\n",
    "                     \"max_n_voices_to_remove\": 1,\n",
    "                     \"prob\": [1],\n",
    "                     \"k\": 1}\n",
    "\n",
    "\n",
    "# train subset\n",
    "pickle_source_path = '../../preprocessed_dataset/datasets_extracted_locally/GrooveMidi/hvo_0.4.2/Processed_On_17_05_2021_at_22_32_hrs'\n",
    "subset_name = 'GrooveMIDI_processed_train'\n",
    "metadata_csv_filename = 'metadata.csv'\n",
    "hvo_pickle_filename = 'hvo_sequence_data.obj'\n",
    "\n",
    "gmd_subsetter = GrooveMidiSubsetter(\n",
    "    pickle_source_path=pickle_source_path,\n",
    "    subset=subset_name,\n",
    "    hvo_pickle_filename=hvo_pickle_filename,\n",
    "    list_of_filter_dicts_for_subsets=[filters],\n",
    ")\n",
    "_, subset_list = gmd_subsetter.create_subsets()\n",
    "\n",
    "subset_info = {\"pickle_source_path\": pickle_source_path,\n",
    "               \"subset\": subset_name,\n",
    "               \"metadata_csv_filename\": metadata_csv_filename,\n",
    "               \"hvo_pickle_filename\": hvo_pickle_filename,\n",
    "               \"filters\": filters}\n",
    "\n",
    "\n",
    "train_data = GrooveMidiDataset(subset=subset_list[0], subset_info=subset_info, mso_parameters=mso_parameters,\n",
    "                                     max_aug_items=100, voices_parameters=voices_parameters)\n",
    "\n",
    "# test \n",
    "pickle_source_path = '../../preprocessed_dataset/datasets_extracted_locally/GrooveMidi/hvo_0.4.2/Processed_On_17_05_2021_at_22_32_hrs'\n",
    "subset_name = 'GrooveMIDI_processed_test'\n",
    "metadata_csv_filename = 'metadata.csv'\n",
    "hvo_pickle_filename = 'hvo_sequence_data.obj'\n",
    "\n",
    "filters = {\"beat_type\": [\"beat\"],\n",
    "           \"time_signature\" : [\"4-4\"],\n",
    "           \"master_id\":[\"drummer9/session1/7\"]}\n",
    "\n",
    "\n",
    "gmd_subsetter = GrooveMidiSubsetter(\n",
    "    pickle_source_path=pickle_source_path,\n",
    "    subset=subset_name,\n",
    "    hvo_pickle_filename=hvo_pickle_filename,\n",
    "    list_of_filter_dicts_for_subsets=[filters],\n",
    ")\n",
    "_, subset_list = gmd_subsetter.create_subsets()\n",
    "\n",
    "subset_info = {\"pickle_source_path\": pickle_source_path,\n",
    "               \"subset\": subset_name,\n",
    "               \"metadata_csv_filename\": metadata_csv_filename,\n",
    "               \"hvo_pickle_filename\": hvo_pickle_filename,\n",
    "               \"filters\": filters}\n",
    "\n",
    "\n",
    "test_data = GrooveMidiDataset(subset=subset_list[0], subset_info=subset_info, mso_parameters=mso_parameters,\n",
    "                                    max_aug_items=100, voices_parameters=voices_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "d_model_enc = 16\n",
    "d_model_dec = 27\n",
    "nhead_enc = 4\n",
    "nhead_dec = 3\n",
    "dim_feedforward = d_model*10\n",
    "dropout = 0.1\n",
    "num_encoder_layers = 5\n",
    "num_decoder_layers = 6\n",
    "max_len=32\n",
    "\n",
    "\n",
    "\n",
    "TM = Transformer(d_model_enc,d_model_dec, nhead_enc, nhead_dec, dim_feedforward, dropout, num_encoder_layers, num_decoder_layers,max_len)\n",
    "\n",
    "\n",
    "\n",
    "model = TM.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    for batch, (X,y,idx) in enumerate(dataloader):\n",
    "        X = X.reshape((32,64,16)) # reorder dimensions\n",
    "        y = y.reshape((32,64,27)) # reorder dimensions\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        y_shifted = torch.zeros([1,64,27])\n",
    "        y_shifted = torch.cat((y_shifted, y), dim=0)\n",
    "        pred = model(X,y_shifted)\n",
    "        print(pred.shape, y.shape)\n",
    "        pred_l = pred.reshape([64,33,27])\n",
    "        y_l = y.reshape([64,32,27])\n",
    "        \n",
    "        # sum 3 different losses\n",
    "        loss = loss_fn(pred_l, y_l)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, idx in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.__len__())\n",
    "print(test_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "x torch.Size([32, 64, 16])\n",
      "memory torch.Size([32, 64, 16])\n",
      "memory_map torch.Size([32, 64, 27])\n",
      "mask torch.Size([33, 33])\n",
      "out torch.Size([33, 64, 27])\n",
      "torch.Size([33, 64, 27]) torch.Size([32, 64, 27])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (64, 27), got torch.Size([64, 32, 27])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-6e6794d39e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-267-072e59d844a8>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpred_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_thesis/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_thesis/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_thesis/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_thesis/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected target size {}, got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (64, 27), got torch.Size([64, 32, 27])"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x7fc7ece46798>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_thesis",
   "language": "python",
   "name": "torch_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
